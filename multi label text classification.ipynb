{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=np.zeros((19000,1),dtype=object)\n",
    "total_data_class=np.zeros((19000,20),dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "events={('AVALANCHES',1):['avalanche','avalanches'],('AVIATION_HAZARD',2):['plane', 'wreckage', 'pilot', 'aviation', 'helicopter', 'aircraft', 'flight'],('ARMED_CONFLICTS',3):['conflict', 'militia', 'militant', 'attack', 'armed', 'fight', 'atrocities', 'weapon',  'war', 'gun', 'arms', 'killing'],\n",
    "       ('CLIMATE_CHANGE',4):['climate', 'environment', 'carbon', 'heat wave', 'cold wave', 'temperature'],('DISEASE_OUTBREAK',5):['disease', 'epidemic', 'pandemic', 'outbreak', 'spread', 'health', 'virus'],\n",
    "       ('EARTHQUAKE',6):['earthquake', 'seismic', 'magnitude', 'epicenter', 'tremor', 'aftershock', 'mainshock', 'quake'],('FIRE',7):['fire', 'forest fire', 'blaze', 'burn'],\n",
    "       ('FLOOD',8):['flood', 'rain','flooding'],('INDUSTRIAL_ACCIDENT',9):['worker','killed','killing','accident','injure','injuring','injured','industry','industries'],('LANDSLIDE',10):['landslide', 'rock fall', 'buried', 'rubble'],('NORMAL_BOMBING',11):['bomb', 'explosive','explosion','bombing'],('RIOTS',12):['riot','riots'],\n",
    "       ('STORM',13):['storm','storms','tornado','hurricane','cyclone','blizzard','hailstorm'],('SHOOT_OUT',14):['shoot-out','shoot_out','shoot','firing','shooting'],('SURGICAL-STRIKES',15):['surgical-strike','army','attack'],('TERRORIST_ATTACK',16):['terrorist','attack','suicide_attack','terrorists'],('TRANSPORT_HAZARD',17):['collision','accidents','accident','crash','crashed','collided'],('TSUNAMI',18):['tsunami','tsunamis'],('VOLCANO',19):['volcano','volcanoes','erruption','errupted','errupt']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert(lst): \n",
    "    res_dct = {lst[i]: 0 for i in range(0, len(lst))} \n",
    "    return res_dct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "18189\n",
      "0 ---->> 201\n",
      "1 ---->> 1415\n",
      "2 ---->> 2912\n",
      "3 ---->> 1079\n",
      "4 ---->> 1156\n",
      "5 ---->> 1698\n",
      "6 ---->> 547\n",
      "7 ---->> 824\n",
      "8 ---->> 3102\n",
      "9 ---->> 271\n",
      "10 ---->> 612\n",
      "11 ---->> 349\n",
      "12 ---->> 998\n",
      "13 ---->> 679\n",
      "14 ---->> 800\n",
      "15 ---->> 981\n",
      "16 ---->> 1385\n",
      "17 ---->> 506\n",
      "18 ---->> 383\n",
      "19 ---->> 5382\n"
     ]
    }
   ],
   "source": [
    "folders=os.listdir('data/english-corpus')\n",
    "k=0\n",
    "for i,folder in enumerate(folders):\n",
    "    files=os.listdir('data/english-corpus/'+folder)\n",
    "    print(i)\n",
    "    for file in files:\n",
    "        \n",
    "        with open('data/english-corpus/'+folder+'/'+file, \"r\" ,errors='ignore') as f:\n",
    "            data=f.read().lower()\n",
    "            data=data.split('\\n')\n",
    "            data='.'.join(data)\n",
    "            total_data[k]=data\n",
    "            ##################################################################################################\n",
    "            ##CHECKING THE CLASS\n",
    "            words=data.split()\n",
    "            \n",
    "            for event,keywords in events.items():\n",
    "                dict_keywords=Convert(keywords)\n",
    "                for word in words:\n",
    "                    try:\n",
    "                        dict_keywords[word]=dict_keywords[word]+1\n",
    "                    except:\n",
    "                        continue\n",
    "                count=sum(dict_keywords.values())\n",
    "                if(count>=3):\n",
    "                    total_data_class[k][event[1]-1]=1\n",
    "                elif(event[1]==14 and count>1):\n",
    "                    total_data_class[k][event[1]-1]=1\n",
    "            #####################################################################################################\n",
    "            if(np.sum(total_data_class[k])==0):\n",
    "                #print(total_data_class[k])\n",
    "                total_data_class[k,19]=1\n",
    "            k=k+1\n",
    "            #print(data)\n",
    "##############################################################################################\n",
    "for i in range(0,np.shape(total_data)[0]):\n",
    "    if(total_data[i]==0):\n",
    "        print(i)\n",
    "        total_data=total_data[:i]\n",
    "        total_data_class=total_data_class[:i]\n",
    "        break\n",
    "##############################################################################################\n",
    "for i in range(0,20):\n",
    "    print(i,'---->>',np.count_nonzero(total_data_class[:,i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "\n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy()\n",
    "# Add 'UNK' to the vocabulary\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts=total_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=tk.texts_to_sequences(train_texts.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pad_sequences(sequence, maxlen=2000, padding='post')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.array(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "train_classes = total_data_class\n",
    "print(total_data_class[14561])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 2000, 69)          4830      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1994, 256)         123904    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1994, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 664, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 658, 256)          459008    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 658, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 219, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 217, 256)          196864    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 217, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 215, 256)          196864    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 215, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 213, 256)          196864    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 213, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 211, 256)          196864    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 211, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 70, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 17920)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              18351104  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                20500     \n",
      "=================================================================\n",
      "Total params: 20,796,402\n",
      "Trainable params: 20,796,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13641\n",
      "17000 17000\n",
      "1189 1189\n"
     ]
    }
   ],
   "source": [
    "input_size = 2000\n",
    "vocab_size = len(tk.word_index)\n",
    "embedding_size = 69\n",
    "conv_layers = [[256, 7, 3],\n",
    "               [256, 7, 3],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, -1],\n",
    "               [256, 3, 3]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = 20\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "# Embedding weights\n",
    "embedding_weights = []  # (70, 69)\n",
    "embedding_weights.append(np.zeros(vocab_size))  # (0, 69)\n",
    "\n",
    "for char, i in tk.word_index.items():  # from index 1 to 69\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i - 1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "\n",
    "embedding_weights = np.array(embedding_weights)\n",
    "print('Load')\n",
    "\n",
    "# Embedding layer Initialization\n",
    "embedding_layer = Embedding(vocab_size + 1,\n",
    "                            embedding_size,\n",
    "                            input_length=input_size,\n",
    "                            weights=[embedding_weights])\n",
    "\n",
    "# Model Construction\n",
    "# Input\n",
    "inputs = Input(shape=(input_size,), name='input', dtype='int64')  # shape=(?, 2000)\n",
    "# Embedding\n",
    "x = embedding_layer(inputs)\n",
    "# Conv\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
    "x = Flatten()(x)  # (None, 8704)\n",
    "# Fully connected layers\n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024\n",
    "    x = Dropout(dropout_p)(x)\n",
    "# Output Layer\n",
    "predictions = Dense(num_of_classes, activation='sigmoid')(x)\n",
    "# Build model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['categorical_accuracy',recall_m,precision_m,f1_m])  # Adam, binary_crossentropy\n",
    "model.summary()\n",
    "\n",
    "# Shuffle\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "ind1=int(len(indices)*0.75)\n",
    "x_train = train_data[indices[:17000]]\n",
    "y_train = train_classes[indices[:17000]]\n",
    "\n",
    "x_test = train_data[indices[17000:]]\n",
    "y_test = train_classes[indices[17000:]]\n",
    "\n",
    "# Training\n",
    "print(ind1)\n",
    "print(len(x_train),len(y_train))\n",
    "print(len(x_test),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17000 samples, validate on 1189 samples\n",
      "Epoch 1/20\n",
      "17000/17000 [==============================] - 67s 4ms/step - loss: 0.2476 - categorical_accuracy: 0.2420 - recall_m: 0.0250 - precision_m: 0.3773 - f1_m: 0.0374 - val_loss: 0.2196 - val_categorical_accuracy: 0.3053 - val_recall_m: 0.0091 - val_precision_m: 0.7231 - val_f1_m: 0.0178\n",
      "Epoch 2/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.2219 - categorical_accuracy: 0.2685 - recall_m: 0.0176 - precision_m: 0.5692 - f1_m: 0.0338 - val_loss: 0.2180 - val_categorical_accuracy: 0.3053 - val_recall_m: 0.0079 - val_precision_m: 0.7805 - val_f1_m: 0.0155\n",
      "Epoch 3/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.2201 - categorical_accuracy: 0.2792 - recall_m: 0.0167 - precision_m: 0.6058 - f1_m: 0.0323 - val_loss: 0.2178 - val_categorical_accuracy: 0.3053 - val_recall_m: 0.0096 - val_precision_m: 0.6352 - val_f1_m: 0.0189\n",
      "Epoch 4/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.2154 - categorical_accuracy: 0.3105 - recall_m: 0.0485 - precision_m: 0.7004 - f1_m: 0.0882 - val_loss: 0.2057 - val_categorical_accuracy: 0.3667 - val_recall_m: 0.1102 - val_precision_m: 0.6846 - val_f1_m: 0.1893\n",
      "Epoch 5/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.1988 - categorical_accuracy: 0.3746 - recall_m: 0.1118 - precision_m: 0.7341 - f1_m: 0.1917 - val_loss: 0.1903 - val_categorical_accuracy: 0.4230 - val_recall_m: 0.1415 - val_precision_m: 0.6985 - val_f1_m: 0.2346\n",
      "Epoch 6/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.2081 - categorical_accuracy: 0.3491 - recall_m: 0.0818 - precision_m: 0.4873 - f1_m: 0.1377 - val_loss: 0.2246 - val_categorical_accuracy: 0.2002 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_f1_m: 0.0000e+00\n",
      "Epoch 7/20\n",
      "17000/17000 [==============================] - 46s 3ms/step - loss: 0.2171 - categorical_accuracy: 0.3036 - recall_m: 0.0274 - precision_m: 0.4522 - f1_m: 0.0496 - val_loss: 0.2023 - val_categorical_accuracy: 0.3768 - val_recall_m: 0.0691 - val_precision_m: 0.8935 - val_f1_m: 0.1274\n",
      "Epoch 8/20\n",
      "17000/17000 [==============================] - 46s 3ms/step - loss: 0.2020 - categorical_accuracy: 0.3598 - recall_m: 0.0912 - precision_m: 0.7614 - f1_m: 0.1616 - val_loss: 0.2014 - val_categorical_accuracy: 0.3818 - val_recall_m: 0.1094 - val_precision_m: 0.6507 - val_f1_m: 0.1861\n",
      "Epoch 9/20\n",
      "17000/17000 [==============================] - 46s 3ms/step - loss: 0.1973 - categorical_accuracy: 0.3754 - recall_m: 0.0983 - precision_m: 0.7743 - f1_m: 0.1730 - val_loss: 0.1897 - val_categorical_accuracy: 0.4188 - val_recall_m: 0.1196 - val_precision_m: 0.7932 - val_f1_m: 0.2061\n",
      "Epoch 10/20\n",
      "17000/17000 [==============================] - 46s 3ms/step - loss: 0.1819 - categorical_accuracy: 0.4444 - recall_m: 0.1486 - precision_m: 0.7713 - f1_m: 0.2467 - val_loss: 0.1819 - val_categorical_accuracy: 0.4340 - val_recall_m: 0.1729 - val_precision_m: 0.7054 - val_f1_m: 0.2765\n",
      "Epoch 11/20\n",
      "17000/17000 [==============================] - 46s 3ms/step - loss: 0.1607 - categorical_accuracy: 0.5146 - recall_m: 0.2522 - precision_m: 0.7177 - f1_m: 0.3708 - val_loss: 0.1446 - val_categorical_accuracy: 0.5627 - val_recall_m: 0.3261 - val_precision_m: 0.7709 - val_f1_m: 0.4565\n",
      "Epoch 12/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.1295 - categorical_accuracy: 0.6196 - recall_m: 0.4487 - precision_m: 0.7538 - f1_m: 0.5603 - val_loss: 0.1236 - val_categorical_accuracy: 0.6409 - val_recall_m: 0.5229 - val_precision_m: 0.7637 - val_f1_m: 0.6201\n",
      "Epoch 13/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.1102 - categorical_accuracy: 0.6762 - recall_m: 0.5503 - precision_m: 0.7845 - f1_m: 0.6458 - val_loss: 0.1096 - val_categorical_accuracy: 0.6627 - val_recall_m: 0.5815 - val_precision_m: 0.7731 - val_f1_m: 0.6634\n",
      "Epoch 14/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.0969 - categorical_accuracy: 0.7079 - recall_m: 0.6146 - precision_m: 0.8162 - f1_m: 0.7002 - val_loss: 0.1056 - val_categorical_accuracy: 0.7048 - val_recall_m: 0.6355 - val_precision_m: 0.7798 - val_f1_m: 0.6999\n",
      "Epoch 15/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.0852 - categorical_accuracy: 0.7367 - recall_m: 0.6727 - precision_m: 0.8351 - f1_m: 0.7442 - val_loss: 0.1030 - val_categorical_accuracy: 0.7199 - val_recall_m: 0.6678 - val_precision_m: 0.8038 - val_f1_m: 0.7293\n",
      "Epoch 16/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.0744 - categorical_accuracy: 0.7612 - recall_m: 0.7172 - precision_m: 0.8490 - f1_m: 0.7768 - val_loss: 0.0959 - val_categorical_accuracy: 0.7267 - val_recall_m: 0.6915 - val_precision_m: 0.7922 - val_f1_m: 0.7383\n",
      "Epoch 17/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.0628 - categorical_accuracy: 0.7915 - recall_m: 0.7649 - precision_m: 0.8722 - f1_m: 0.8145 - val_loss: 0.0983 - val_categorical_accuracy: 0.7174 - val_recall_m: 0.6844 - val_precision_m: 0.8138 - val_f1_m: 0.7432\n",
      "Epoch 18/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.0545 - categorical_accuracy: 0.8069 - recall_m: 0.8000 - precision_m: 0.8911 - f1_m: 0.8425 - val_loss: 0.1005 - val_categorical_accuracy: 0.7157 - val_recall_m: 0.7270 - val_precision_m: 0.7816 - val_f1_m: 0.7532\n",
      "Epoch 19/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.0448 - categorical_accuracy: 0.8262 - recall_m: 0.8370 - precision_m: 0.9070 - f1_m: 0.8702 - val_loss: 0.1081 - val_categorical_accuracy: 0.7250 - val_recall_m: 0.7293 - val_precision_m: 0.7881 - val_f1_m: 0.7572\n",
      "Epoch 20/20\n",
      "17000/17000 [==============================] - 47s 3ms/step - loss: 0.0418 - categorical_accuracy: 0.8331 - recall_m: 0.8533 - precision_m: 0.9121 - f1_m: 0.8812 - val_loss: 0.1027 - val_categorical_accuracy: 0.7393 - val_recall_m: 0.7473 - val_precision_m: 0.7992 - val_f1_m: 0.7721\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff05e231e10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "def xml_parser(path) :\n",
    "    #Parsing XML Data\n",
    "    event_dict = {} \n",
    "    arg_dict = {}\n",
    "    event_list = ['NATURAL_EVENT','MAN_MADE_EVENT']\n",
    "    arg_list = ['NATURAL_EVENT','MAN_MADE_EVENT','TIME-ARG','PLACE-ARG','PARTICIPANT-ARG', 'EPICENTRE-ARG', 'INTENSITY-ARG', 'MAGNITUDE-ARG', 'AFTER_EFFECTS-ARG','CASUALTIES-ARG','REASON-ARG'] \n",
    "    #arg_list = ['NATURAL_EVENT','MAN_MADE_EVENT','TIME-ARG','PLACE-ARG','PARTICIPANT-ARG','AFTER_EFFECTS-ARG','CASUALTIES-ARG','REASON-ARG'] \n",
    "    count = {}\n",
    "    sent = []\n",
    "    tags = {}\n",
    "    doc_events = {}\n",
    "    \n",
    "    \n",
    "    import os\n",
    "    for file in os.listdir(path): \n",
    "        \n",
    "        #print(\"-------------------------------------------------------------------------------\") \n",
    "        filename = path + file \n",
    "        sent.append(get_text(filename))\n",
    "        #full_text.append(''.join(sentence_split(filename)))\n",
    "        tree = ET.parse(filename)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        \n",
    "        for tag in event_list: \n",
    "            for event in tree.iter(tag): \n",
    "                text = ''\n",
    "\n",
    "                for i in range(len(event)):\n",
    "                    if type(event[i].text) == str: \n",
    "                            text += event[i].text \n",
    "\n",
    "                    if event[i].tag in event_list:\n",
    "                        for j in range(len(event[i])):\n",
    "                            if type(event[i][j].text) == str: \n",
    "                                text += event[i][j].text\n",
    "                #print('------------yoyo-----------------')            \n",
    "\n",
    "\n",
    "                if tag == 'NATURAL_EVENT' or tag == 'MAN_MADE_EVENT':\n",
    "                    #key = event id - filename\n",
    "                    \n",
    "                    label = event.attrib['TYPE']\n",
    "                    if label == 'CYCLONE' or label == 'BLIZZARD' or label == 'TORNADO' or label == 'HURRICANE' or label == 'HAIL_STORMS' or label == 'STORM':\n",
    "                        label = 'STORM'\n",
    "                    if label == 'SEISMIC_RISK':\n",
    "                        label = \"Earthquake\"\n",
    "                    if label == 'ROCK_FALL':\n",
    "                        label ='LAND_SLIDE'\n",
    "                    if label == 'FOREST_FIRE':\n",
    "                        label = 'FIRE'\n",
    "                    if label == 'AVALANCHES':\n",
    "                        label = 'LAND_SLIDE'\n",
    "                    if label == 'ACCIDENTS' or label == 'VEHICULAR_COLLISION' or label == 'TRAIN_COLLISION':\n",
    "                        label = 'TRANSPORT_HAZARDS'\n",
    "                    if label == 'SUICIDE_ATTACK':\n",
    "                        label ='TERRORIST_ATTACK'\n",
    "                    if label == 'CRIME' or label == 'LIMNIC_ERRUPTIONS' or label == 'FAMINE' or label == 'HEAVY_RAINFALL' or label =='DROUGHT':\n",
    "                        label ='MISCELLANEOUS'\n",
    "                    if label == 'HEAT_WAVE' or label == 'COLD_WAVE':\n",
    "                        label = 'CLIMATE_CHANGE'\n",
    "                    if label == 'EPIDEMIC' or label == 'PANDEMIC' :\n",
    "                        label = 'DISEASE_OUTBREAK'\n",
    "                    \n",
    "                    event_dict[event.attrib['ID'] + '-' + file ] = (label, text)\n",
    "                \n",
    "                    if file not in doc_events:\n",
    "                        doc_events[file] = [label]\n",
    "                    else:\n",
    "                        if label not in doc_events[file]:\n",
    "                            doc_events[file].append(label)\n",
    "                            \n",
    "            \n",
    "    \n",
    "        try:\n",
    "            for type_ in list(set(doc_events[file])):\n",
    "                if type_ not in count:\n",
    "                    count[type_] = 1\n",
    "                else:\n",
    "                    count[type_] = count[type_] + 1\n",
    "        except:\n",
    "            print(file)\n",
    "            continue\n",
    "            \n",
    "        '''try:\n",
    "            print(doc_events[file])\n",
    "        except:\n",
    "            continue\n",
    "        '''\n",
    "                    \n",
    "        \n",
    "        tagged = []\n",
    "        for child in root:\n",
    "\n",
    "            for child1 in child:\n",
    "                link = ''\n",
    "                text = ''\n",
    "\n",
    "                if child1.text != None:\n",
    "                    tagged.append(('O', child1.text.strip())) \n",
    "\n",
    "                if child1.tag in arg_list:\n",
    "                    for child2 in child1:\n",
    "\n",
    "                        if child2.tag == 'LINK' :\n",
    "                            link = child2.attrib['EVENT_ARG']   #the id of the event it is linked to\n",
    "                            tag = child1.tag\n",
    "                            begin_flag = 0\n",
    "                            continue\n",
    "\n",
    "                        elif child2.tag == 'ASSOCIATED-EVENT-LINK':\n",
    "                            link = child2.attrib['EVENT_ID']\n",
    "                            #print('yo', file)\n",
    "                            tag = 'REASON-ARG'\n",
    "                            begin_flag = 0\n",
    "                            continue\n",
    "                            \n",
    "                        elif child1.tag in event_list and child2.text != None and link == '':\n",
    "                            tagged.append(('O', child2.text.strip()))   #events\n",
    "\n",
    "                        if link != '' :\n",
    "                            try:\n",
    "\n",
    "                                text = text + child2.text\n",
    "                                \n",
    "                                #key = arg id - filename\n",
    "                                arg_dict[child1.attrib['ID'] + '-' + file] = (link, tag, text)    \n",
    "                                \n",
    "                                #BIO tagging\n",
    "                                if begin_flag == 0 :\n",
    "                                    tagged.append(('B' + '_' + tag + '__' + event_dict[link + '-' + file][0] + '__' + event_dict[link + '-' + file][1].strip(), child2.text.strip()))\n",
    "                                    begin_flag = 1\n",
    "                                \n",
    "                                else :\n",
    "                                    tagged.append(('I' + '_' + tag + '__' + event_dict[link + '-' + file][0] + '__' + event_dict[link + '-' + file][1].strip() , child2.text.strip()))\n",
    "                            \n",
    "                            \n",
    "                            except:\n",
    "                                continue\n",
    "                       \n",
    "        tags[file] = tagged\n",
    "        \n",
    "    \n",
    "    total = {}\n",
    "    fault = []\n",
    "    for k, v in arg_dict.items():\n",
    "        #key = event id + arg id + filename\n",
    "        key = v[0] + '-' + k.split('-')[0] + '-' + k.split('-')[1]\n",
    "        total[key] = {}\n",
    "        try:\n",
    "            total[key]['event_type'] = event_dict[v[0] + '-' + k.split('-')[1]][0]\n",
    "            total[key]['event_trigger'] = event_dict[v[0] + '-' + k.split('-')[1]][1]\n",
    "            total[key]['arg_type'] = v[1]\n",
    "            total[key]['arg_trigger'] = v[2]\n",
    "        except:\n",
    "            fault.append((v[0] + '-' + k.split('-')[1]))\n",
    "            \n",
    "    #print('FAULTS:', fault)\n",
    "            \n",
    "            \n",
    "    print(count)\n",
    "    return tags, doc_events, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "def get_text(filename):\n",
    "    \n",
    "    tree = etree.parse(filename)\n",
    "    notags = etree.tostring(tree, encoding='unicode', method='text')\n",
    "    \n",
    "    return notags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "XMLSyntaxError",
     "evalue": "Start tag expected, '<' not found, line 2, column 1 (36.xml, line 2)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2963\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-36-5ee6f569db54>\"\u001b[0m, line \u001b[1;32m3\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    doc,a,b=xml_parser(\"data/newdata/bengali/fire/\")\n",
      "  File \u001b[1;32m\"<ipython-input-21-662e224a7a9a>\"\u001b[0m, line \u001b[1;32m20\u001b[0m, in \u001b[1;35mxml_parser\u001b[0m\n    sent.append(get_text(filename))\n",
      "  File \u001b[1;32m\"<ipython-input-22-09dc064e2b39>\"\u001b[0m, line \u001b[1;32m4\u001b[0m, in \u001b[1;35mget_text\u001b[0m\n    tree = etree.parse(filename)\n",
      "  File \u001b[1;32m\"src/lxml/etree.pyx\"\u001b[0m, line \u001b[1;32m3426\u001b[0m, in \u001b[1;35mlxml.etree.parse\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1839\u001b[0m, in \u001b[1;35mlxml.etree._parseDocument\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1865\u001b[0m, in \u001b[1;35mlxml.etree._parseDocumentFromURL\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1769\u001b[0m, in \u001b[1;35mlxml.etree._parseDocFromFile\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m1162\u001b[0m, in \u001b[1;35mlxml.etree._BaseParser._parseDocFromFile\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m600\u001b[0m, in \u001b[1;35mlxml.etree._ParserContext._handleParseResultDoc\u001b[0m\n",
      "  File \u001b[1;32m\"src/lxml/parser.pxi\"\u001b[0m, line \u001b[1;32m710\u001b[0m, in \u001b[1;35mlxml.etree._handleParseResult\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"src/lxml/parser.pxi\"\u001b[0;36m, line \u001b[0;32m639\u001b[0;36m, in \u001b[0;35mlxml.etree._raiseParseError\u001b[0;36m\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"data/newdata/bengali/fire/36.xml\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    # <TITLE>লেনিন সরণিতে বহুতলে আগুন, ঘটনাস্থলে দমকলের ছ’টি ইঞ্জিন, হতাহতের খবর নেই</TITLE>\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mXMLSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Start tag expected, '<' not found, line 2, column 1\n"
     ]
    }
   ],
   "source": [
    "test_data=np.zeros((206,1),dtype=object)\n",
    "total_count=0\n",
    "doc,a,b=xml_parser(\"English/Test/\")\n",
    "i=0\n",
    "for file_name,d in doc.items():\n",
    "    total_count=total_count+1\n",
    "    text=\"\"\n",
    "    for x in d:\n",
    "        text=text+\" \"+x[1]\n",
    "    test_data[i]=text\n",
    "    \n",
    "    i=i+1\n",
    "doc,a,b=xml_parser(\"English/Train/\")\n",
    "i=0\n",
    "for file_name,d in doc.items():\n",
    "    total_count=total_count+1\n",
    "    text=\"\"\n",
    "    for x in d:\n",
    "        text=text+\" \"+x[1]\n",
    "    test_data=np.append(test_data,np.array([[text]]),axis=0)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ---->> 9\n",
      "1 ---->> 25\n",
      "2 ---->> 70\n",
      "3 ---->> 5\n",
      "4 ---->> 23\n",
      "5 ---->> 59\n",
      "6 ---->> 178\n",
      "7 ---->> 25\n",
      "8 ---->> 624\n",
      "9 ---->> 14\n",
      "10 ---->> 35\n",
      "11 ---->> 1\n",
      "12 ---->> 43\n",
      "13 ---->> 20\n",
      "14 ---->> 37\n",
      "15 ---->> 36\n",
      "16 ---->> 289\n",
      "17 ---->> 10\n",
      "18 ---->> 10\n",
      "19 ---->> 173\n"
     ]
    }
   ],
   "source": [
    "Test_data_classes=np.zeros((len(test_data),20),dtype=float)\n",
    "for i in range(0,len(test_data)):\n",
    "    count=0\n",
    "    words=test_data[i][0].split()\n",
    "\n",
    "    for event,keywords in events.items():\n",
    "        dict_keywords=Convert(keywords)\n",
    "        for word in words:\n",
    "            try:\n",
    "                dict_keywords[word]=dict_keywords[word]+1\n",
    "            except:\n",
    "                continue\n",
    "        count=sum(dict_keywords.values())\n",
    "        if(count>=2):\n",
    "            Test_data_classes[i][event[1]-1]=1\n",
    "        elif(event[1]==14 and count>1):\n",
    "            Test_data_classes[i][event[1]-1]=1\n",
    "    #####################################################################################################\n",
    "    if(np.sum(Test_data_classes[i])==0):\n",
    "        #print(total_data_class[i])\n",
    "        Test_data_classes[i,19]=1\n",
    "\n",
    "for i in range(0,20):\n",
    "    print(i,'---->>',np.count_nonzero(Test_data_classes[:,i]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence2=tk.texts_to_sequences(test_data.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_data = pad_sequences(sequence2, maxlen=2000, padding='post')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_data=np.array(Test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_classes = Test_data_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1034/1034 [==============================] - 1s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23139815914446427,\n",
       " 0.6363636365942152,\n",
       " 0.5965370736454165,\n",
       " 0.7642563508834322,\n",
       " 0.6688848353446798]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(Test_data,Test_classes,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perlabel_accuracy(y,yp):\n",
    "    \n",
    "    for col in range (0,20):\n",
    "        metriccount=[0,0,0,0]\n",
    "        trc=y[:,col]\n",
    "        tec=yp[:,col]\n",
    "        for i in range(0,len(y)):\n",
    "            if(trc[i]==1.0 and tec[i]==1.0):\n",
    "                metriccount[0]=metriccount[0]+1\n",
    "            if(trc[i]==0.0 and tec[i]==0.0):\n",
    "                metriccount[1]=metriccount[1]+1\n",
    "            if(trc[i]==0.0 and tec[i]==1.0):\n",
    "                metriccount[2]=metriccount[2]+1\n",
    "            if(trc[i]==1.0 and tec[i]==0.0):\n",
    "                metriccount[3]=metriccount[3]+1\n",
    "        try:\n",
    "            acc=(metriccount[0]+metriccount[1])/(metriccount[0]+metriccount[1]+metriccount[2]+metriccount[3])\n",
    "        except:\n",
    "            acc=0\n",
    "        try:\n",
    "            recall=metriccount[0]/(metriccount[0]+metriccount[3])\n",
    "        except:\n",
    "            recall=0\n",
    "        try:\n",
    "            precision=metriccount[0]/(metriccount[0]+metriccount[2])\n",
    "        except:\n",
    "            precison=0\n",
    "        try:\n",
    "            f1=2*recall*precision/(recall+precision)\n",
    "        except:\n",
    "            f1=0\n",
    "        print(str(col)+'-->'+str(acc)+'--'+str(recall)+'--'+str(precision)+'--'+str(f1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "yp1=model.predict(x_train[:100])\n",
    "yp1[yp1>=0.5]=1.0\n",
    "yp1[yp1<0.5]=0.0\n",
    "perlabel_accuracy(y_train[:100],yp1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
